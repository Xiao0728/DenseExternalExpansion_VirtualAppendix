{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "artificial-covering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 19 18:01:25 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:43:00.0 Off |                  Off |\r\n",
      "| 30%   36C    P8    31W / 300W |  48111MiB / 48685MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "plastic-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "assert faiss.get_num_gpus() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sustained-austria",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.8.0 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-usage",
   "metadata": {},
   "source": [
    "# Indexing for the Target Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_colbert.indexing import ColBERTIndexer\n",
    "index_root=\"/nfs/indices/BEIR/nfcorpus\"\n",
    "index_name=\"nfcorpus_colbertIndex\"\n",
    "dataset = pt.get_dataset(\"irds:beir/nfcorpus\")\n",
    "\n",
    "# default 150, stride 75\n",
    "checkpoint=\"http://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/colbert_model_checkpoint.zip\"\n",
    "\n",
    "indexer =  pt.text.sliding(text_attr=\"text\", prepend_attr='title') >> ColBERTIndexer(checkpoint,index_root, index_name, chunksize=20)\n",
    "indexer.index(dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-geometry",
   "metadata": {},
   "source": [
    "# Indexing for the External Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_colbert.indexing import ColBERTIndexer\n",
    "index_root=\"/nfs/indices/colbert_passage\"\"\n",
    "index_name=\"external_colbertIndex\"\n",
    "dataset = pt.get_dataset(\"trec-deep-learning-passages\")\n",
    "checkpoint=\"http://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/colbert_model_checkpoint.zip\"\n",
    "\n",
    "\n",
    "indexer = ColBERTIndexer(checkpoint,index_root, index_name, chunksize=20)\n",
    "indexer.index(dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-delta",
   "metadata": {},
   "source": [
    "# Loading query set\n",
    "- Choose the type of query set you would like to load, this demo shows the example code for loading the NFCorpus  query set:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "declared-auckland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_nfcorpus=pt.get_dataset(\"irds:beir/nfcorpus/test\").get_topics()\n",
    "qrels_nfcorpus=pt.get_dataset(\"irds:beir/nfcorpus/test\").get_qrels()\n",
    "len(topics_nfcorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-sociology",
   "metadata": {},
   "source": [
    "# ColBERT-PRF on Target Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "respiratory-skill",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 16, 13:24:23] #> Loading model checkpoint.\n",
      "[May 16, 13:24:23] #> Loading checkpoint /nfs/xiao/GOOD_MODELS/colbert.dnn\n",
      "[May 16, 13:24:24] #> checkpoint['epoch'] = 0\n",
      "[May 16, 13:24:24] #> checkpoint['batch'] = 44500\n",
      "[May 16, 13:24:25] #> Loading the FAISS index from /nfs/indices/BEIR/nfcorpus/nfcorpus_colbertIndex/ivfpq.100.faiss ..\n",
      "[May 16, 13:24:25] #> Building the emb2pid mapping..\n",
      "[May 16, 13:24:25] len(self.emb2pid) = 1064705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading index shards to memory:   0%|          | 0/1 [00:00<?, ?shard/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reranking index, memtype=mem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading index shards to memory: 100%|██████████| 1/1 [00:00<00:00,  1.84shard/s]\n"
     ]
    }
   ],
   "source": [
    "import pyterrier_colbert.ranking\n",
    "pytcolbertTgt = pyterrier_colbert.ranking.ColBERTFactory(\"http://www.dcs.gla.ac.uk/~craigm/colbert.dnn.zip\", \\\n",
    "                                                      \"/nfs/indices/BEIR/nfcorpus\", \\\n",
    "                                                      \"nfcorpus_colbertIndex\",faiss_partitions=100,memtype='mem')\n",
    "pytcolbertTgt.faiss_index_on_gpu = False\n",
    "dense_e2eTgt = pytcolbertTgt.set_retrieve() >> pytcolbertTgt.index_scorer(query_encoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "simple-glucose",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30522/30522 [00:00<00:00, 259821.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from pyterrier_colbert.ranking import ColbertPRF\n",
    "dense_e2eTgt = pytcolbertTgt.set_retrieve() >> pytcolbertTgt.index_scorer(query_encoded=True)\n",
    "PRFTgt = dense_e2eTgt %10 \\\n",
    ">> ColbertPRF(pytcolbertTgt, k=24, fb_docs=3, fb_embs=10, beta=1)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "medieval-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n"
     ]
    }
   ],
   "source": [
    "print(len(topics_nfcorpus))\n",
    "res = PRFTgt(topics_nfcorpus)\n",
    "import pandas as pd\n",
    "pd.to_pickle(res,\"/path_to_save/tgt/Tgt_BEIR.nfcorpus.embs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-colon",
   "metadata": {},
   "source": [
    "#  ColBERT-PRF on External Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coated-sense",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 24, 13:21:35] #> Loading model checkpoint.\n",
      "[Apr 24, 13:21:35] #> Loading checkpoint /nfs/xiao/GOOD_MODELS/colbert.dnn\n",
      "[Apr 24, 13:21:36] #> checkpoint['epoch'] = 0\n",
      "[Apr 24, 13:21:36] #> checkpoint['batch'] = 44500\n",
      "[Apr 24, 13:21:37] #> Loading the FAISS index from /nfs/craigm/indices/colbert_passage/index_name3/ivfpq.100.faiss ..\n",
      "[Apr 24, 13:22:03] #> Building the emb2pid mapping..\n",
      "[Apr 24, 13:22:42] len(self.emb2pid) = 687989391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading index shards to memory:   0%|          | 0/24 [00:00<?, ?shard/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reranking index, memtype=mem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading index shards to memory: 100%|██████████| 24/24 [02:53<00:00,  7.21s/shard]\n"
     ]
    }
   ],
   "source": [
    "from pyterrier_colbert.ranking import ColBERTFactory\n",
    "\n",
    "import pyterrier_colbert.ranking\n",
    "pytcolbert = pyterrier_colbert.ranking.ColBERTFactory(\"http://www.dcs.gla.ac.uk/~craigm/colbert.dnn.zip\", \\\n",
    "                                                      \"/nfs/indices/colbert_passage\", \\\n",
    "                                                      \"external_colbertIndex\",faiss_partitions=100,memtype='mem')\n",
    "pytcolbert.faiss_index_on_gpu = False\n",
    "dense_e2e = pytcolbert.set_retrieve() >> pytcolbert.index_scorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "heated-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30522/30522 [00:00<00:00, 216550.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PRF_ext = dense_e2e %10 >> ColbertPRF(pytcolbert, k=24, fb_docs=3, fb_embs=10, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "arbitrary-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyterrier_colbert/ranking.py:697: UserWarning: index_scorer() used with query_encoded=False, but query_embs column present in input. Should you use query_encoded=True?\n",
      "  warn(\"index_scorer() used with query_encoded=False, but query_embs column present in input. Should you use query_encoded=True?\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docno</th>\n",
       "      <th>query</th>\n",
       "      <th>query_embs</th>\n",
       "      <th>query_toks</th>\n",
       "      <th>query_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156493</td>\n",
       "      <td>3288601</td>\n",
       "      <td>do goldfish grow</td>\n",
       "      <td>[[tensor(0.0995), tensor(0.1051), tensor(-0.05...</td>\n",
       "      <td>[bowls, ##´, ##fish, kept, gold, fish, grow, f...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid    docno             query  \\\n",
       "0  156493  3288601  do goldfish grow   \n",
       "\n",
       "                                          query_embs  \\\n",
       "0  [[tensor(0.0995), tensor(0.1051), tensor(-0.05...   \n",
       "\n",
       "                                          query_toks  \\\n",
       "0  [bowls, ##´, ##fish, kept, gold, fish, grow, f...   \n",
       "\n",
       "                                       query_weights  \n",
       "0  [tensor(1.), tensor(1.), tensor(1.), tensor(1....  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRF_ext(topics2019.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(topics_nfscorpus))\n",
    "res = PRF_ext(topics_nfscorpus)\n",
    "import pandas as pd\n",
    "pd.to_pickle(res,\"/path_to_save/ext/External_BEIR.nfscorpus.embs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-characterization",
   "metadata": {},
   "source": [
    "# External Dense (ColBERTT-PRF) Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "swiss-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res_tgt = pd.read_pickle(\"/path_to_save/tgt/Tgt_BEIR.nfcorpus.embs.pkl\")\n",
    "res_tgtTrans = pt.transformer.SourceTransformer(res_tgt)\n",
    "res_ext = pd.read_pickle(\"/path_to_save/ext/External_BEIR.nfscorpus.embs.pkl\")\n",
    "res_extTrans = pt.transformer.SourceTransformer(res_ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "protected-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(res_tgt, res_ext):\n",
    "    import torch\n",
    "    first_row = res_ext.iloc[0]\n",
    "    \n",
    "    # concatenate the external embeddings to the target query embeddings \n",
    "    newemb = torch.cat((res_tgt.iloc[0].query_embs,res_ext.iloc[0].query_embs[32:]),0)\n",
    "\n",
    "    # the weights column defines important of each query embedding\n",
    "    newweights = torch.cat((res_tgt.iloc[0].query_weights, res_ext.iloc[0].query_weights[32:]),0)\n",
    "    newtoks = res_tgt.iloc[0].query_toks+ (res_ext.iloc[0].query_toks)\n",
    "\n",
    "    # generate the revised query dataframe row\n",
    "    rtr = pd.DataFrame([\n",
    "        [first_row.qid, \n",
    "         first_row.docno,\n",
    "         first_row.query, \n",
    "         newemb, \n",
    "         newtoks, \n",
    "         newweights]\n",
    "        ],\n",
    "        columns=[\"qid\",\"docno\", \"query\", \"query_embs\", \"query_toks\", \"query_weights\"])\n",
    "    return rtr\n",
    "def mergePRF(res_tgt_input, res_ext_input):\n",
    "    rtrMerge = pd.DataFrame(columns = [\"qid\",\"docno\", \"query\", \"query_embs\", \"query_toks\", \"query_weights\"])\n",
    "    qidlist = res_ext_input.qid.unique()\n",
    "    for qid in qidlist:\n",
    "        res_tgt = res_tgt_input[res_tgt_input[\"qid\"] == qid]\n",
    "        res_ext = res_ext_input[res_ext_input[\"qid\"] == qid]\n",
    "        new_query_df = transform(res_tgt, res_ext)\n",
    "        rtrMerge = rtrMerge.append(new_query_df)\n",
    "    return rtrMerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "standard-prophet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docno</th>\n",
       "      <th>query</th>\n",
       "      <th>query_embs</th>\n",
       "      <th>query_toks</th>\n",
       "      <th>query_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLAIN-1008</td>\n",
       "      <td>4557512</td>\n",
       "      <td>deafness</td>\n",
       "      <td>[[tensor(-0.0544), tensor(0.0031), tensor(0.01...</td>\n",
       "      <td>[##ancy, disability, prevented, ##eti, ##dine,...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLAIN-1018</td>\n",
       "      <td>8568028</td>\n",
       "      <td>dha</td>\n",
       "      <td>[[tensor(-0.0804), tensor(0.1422), tensor(0.09...</td>\n",
       "      <td>[##gal, dh, ##xa, ##he, al, development, condu...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLAIN-102</td>\n",
       "      <td>1331575</td>\n",
       "      <td>stopping heart disease in childhood</td>\n",
       "      <td>[[tensor(0.0095), tensor(0.1305), tensor(0.048...</td>\n",
       "      <td>[##care, stat, childhood, eating, ##8, glucose...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLAIN-1028</td>\n",
       "      <td>755539</td>\n",
       "      <td>dietary scoring</td>\n",
       "      <td>[[tensor(-0.0138), tensor(0.0760), tensor(0.06...</td>\n",
       "      <td>[residues, ##static, pest, sd, score, ##i, 40,...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLAIN-1039</td>\n",
       "      <td>2593757</td>\n",
       "      <td>domoic acid</td>\n",
       "      <td>[[tensor(0.1494), tensor(0.0146), tensor(0.119...</td>\n",
       "      <td>[dom, wildlife, coma, ##oic, shell, poisoning,...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid    docno                                query  \\\n",
       "0  PLAIN-1008  4557512                             deafness   \n",
       "0  PLAIN-1018  8568028                                  dha   \n",
       "0   PLAIN-102  1331575  stopping heart disease in childhood   \n",
       "0  PLAIN-1028   755539                      dietary scoring   \n",
       "0  PLAIN-1039  2593757                          domoic acid   \n",
       "\n",
       "                                          query_embs  \\\n",
       "0  [[tensor(-0.0544), tensor(0.0031), tensor(0.01...   \n",
       "0  [[tensor(-0.0804), tensor(0.1422), tensor(0.09...   \n",
       "0  [[tensor(0.0095), tensor(0.1305), tensor(0.048...   \n",
       "0  [[tensor(-0.0138), tensor(0.0760), tensor(0.06...   \n",
       "0  [[tensor(0.1494), tensor(0.0146), tensor(0.119...   \n",
       "\n",
       "                                          query_toks  \\\n",
       "0  [##ancy, disability, prevented, ##eti, ##dine,...   \n",
       "0  [##gal, dh, ##xa, ##he, al, development, condu...   \n",
       "0  [##care, stat, childhood, eating, ##8, glucose...   \n",
       "0  [residues, ##static, pest, sd, score, ##i, 40,...   \n",
       "0  [dom, wildlife, coma, ##oic, shell, poisoning,...   \n",
       "\n",
       "                                       query_weights  \n",
       "0  [tensor(1.), tensor(1.), tensor(1.), tensor(1....  \n",
       "0  [tensor(1.), tensor(1.), tensor(1.), tensor(1....  \n",
       "0  [tensor(1.), tensor(1.), tensor(1.), tensor(1....  \n",
       "0  [tensor(1.), tensor(1.), tensor(1.), tensor(1....  \n",
       "0  [tensor(1.), tensor(1.), tensor(1.), tensor(1....  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "resMerge = mergePRF(res_tgt, res_ext) \n",
    "resMerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "metallic-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resMerge = pd.read_csv(path_resMErge)\n",
    "MergeTrans = pt.transformer.SourceTransformer(resMerge)\n",
    "pipeCE2_ranker = (MergeTrans \n",
    "                  >> pytcolbertTgt.set_retrieve(query_encoded=True) \n",
    "                  >> (pytcolbertTgt.index_scorer(query_encoded=True, add_ranks=True, batch_size=5000)\n",
    "                  >> pt.text.max_passage() % 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "liable-asbestos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ndcg_cut_10': 0.33206571694278414,\n",
       " 'ndcg_cut_20': 0.30744815120916363,\n",
       " 'ndcg_cut_1000': 0.3834315067045998}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(len(topics_nfcorpus))\n",
    "res = pd.concat([resPart for resPart in pipeCE2_ranker.transform_gen(topics_nfcorpus, batch_size=100)])\n",
    "pt.io.write_results(res, \"/path_to_save/CE.nfcorpus.res.gz\")\n",
    "evalMeasuresDict = pt.Utils.evaluate(res,qrels_nfcorpus,metrics=[\"ndcg_cut_10\",\"ndcg_cut_20\",\"ndcg_cut_1000\"])\n",
    "evalMeasuresDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-armenia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
